{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e266bc7e-4584-47e1-8136-f455b9a9e6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368ec791-651d-4a20-9be9-c4a26200d6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-by-example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcac4c-1a22-43d0-8867-73f983ec4adc",
   "metadata": {
    "tags": []
   },
   "source": [
    "creates an empty RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd5e789-9581-4940-a3b8-12eebbcc59b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emptyRDD = spark.sparkContext.emptyRDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4dbba47-92b3-4c59-aaac-1aff2020a494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "print(emptyRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bffc80-4d06-43bb-867c-623a767c7cae",
   "metadata": {
    "tags": []
   },
   "source": [
    "alternatively an empty RDD can be created by using parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a0dce6-e8a6-4125-a3ce-677d2bd62022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0384b21-816c-4c33-85c1-86a5d48ca821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287\n"
     ]
    }
   ],
   "source": [
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e73e05-8df2-40f1-955c-c8d7ee4d184f",
   "metadata": {
    "tags": []
   },
   "source": [
    "create schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cff7867-01ae-4bba-9e4f-f76e77762211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c625d3f-2f0b-4a4e-b1cc-4d4ea723ebff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creates schema structure\n",
    "schema = StructType([\n",
    "    StructField('firstname', StringType(), True),\n",
    "    StructField('middlename', StringType(), True),\n",
    "    StructField('lastname', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b12d66-9c31-477d-b10f-3633a411396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD, schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4a52cb-0221-43ed-8475-c67d7f53568c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3414274-0330-43d6-a9a1-84b7e7c36c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create empty Dataframe directly\n",
    "df2 = spark.createDataFrame([], schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f02caebc-9dcc-4ffc-9b65-7ac86c5889db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create empty Dataframe with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d7c07d-472c-4445-a6cd-977d29edff7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create PySpark RDD \n",
    "# we will create an RDD by passing Python list object to `sparkContext.parallelize()` function.\n",
    "# in PySpark when you have data in a list means that you have your collection in PySpark Driver memory.\n",
    "# When you create an RDD this collection is going to be parallelized.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark-by-example').getOrCreate()\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef470d2f-19a1-431f-ace9-d40c81d7afca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "# by default, `toDF()` function creates column names as \"_1\" and \"_2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882f5982-a6fb-4b75-a3ee-051be89de0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `toDF()` has another signature that takes arguments to define column names as shown below\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f0a9e-e6fe-4330-80aa-18cb8462ebf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "2.2 Using PySpark `createDataFrame()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4312a-3b67-490f-b501-53439616f5ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "`SparkSession` class provides `createDataFrame()` method to create DataFrame and it takes rdd object as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde7cfe0-35b5-45f7-a479-ceb7b86d8a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4db33-6225-4c1a-aa42-7eb4eb614268",
   "metadata": {},
   "source": [
    "2.3 Using `createDataFrame()` with StructType schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8b967-1673-426b-9f49-de14ce97b18a",
   "metadata": {},
   "source": [
    "When you infer the schema, by default the datatype of the columns is derived from the data and set's nullable to true\n",
    "for all columns. We can change this behavior by supplying schema using StructType - where we can specify a column name\n",
    "data tyoe and nullable for each field/column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "926c8247-947a-499a-b79f-dd908c16615c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "deptSchema = StructType([\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27936566-75a5-4219-b7e7-dcf646783eff",
   "metadata": {},
   "source": [
    "3. Complete Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af9b44-2682-4024-9478-e68a11a457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('spark-by-example').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10), (\"Marketing\", 20), (\"Sales\", 30), (\"IT\", 40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
