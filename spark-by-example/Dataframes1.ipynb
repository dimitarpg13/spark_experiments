{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e266bc7e-4584-47e1-8136-f455b9a9e6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368ec791-651d-4a20-9be9-c4a26200d6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-by-example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65afb110-ba2c-4f54-9a7c-7f6b0ac7e8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creates an empty RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd5e789-9581-4940-a3b8-12eebbcc59b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emptyRDD = spark.sparkContext.emptyRDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4dbba47-92b3-4c59-aaac-1aff2020a494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "print(emptyRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6052774c-e987-410f-9d55-eb0282e92ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alternatively an empty RDD can be created by using parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a0dce6-e8a6-4125-a3ce-677d2bd62022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0384b21-816c-4c33-85c1-86a5d48ca821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287\n"
     ]
    }
   ],
   "source": [
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a33539e9-6b70-4493-a37b-b23f88a54636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create schema \n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c625d3f-2f0b-4a4e-b1cc-4d4ea723ebff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create schema \n",
    "schema = StructType([\n",
    "    StructField('firstname', StringType(), True),\n",
    "    StructField('middlename', StringType(), True),\n",
    "    StructField('lastname', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b12d66-9c31-477d-b10f-3633a411396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD, schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4a52cb-0221-43ed-8475-c67d7f53568c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3414274-0330-43d6-a9a1-84b7e7c36c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create empty Dataframe directly\n",
    "df2 = spark.createDataFrame([], schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f02caebc-9dcc-4ffc-9b65-7ac86c5889db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create empty Dataframe with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d7c07d-472c-4445-a6cd-977d29edff7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create PySpark RDD \n",
    "# we will create an RDD by passing Python list object to `sparkContext.parallelize()` function.\n",
    "# in PySpark when you have data in a list means that you have your collection in PySpark Driver memory.\n",
    "# When you create an RDD this collection is going to be parallelized.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark-by-example').getOrCreate()\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef470d2f-19a1-431f-ace9-d40c81d7afca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "# by default, `toDF()` function creates column names as \"_1\" and \"_2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882f5982-a6fb-4b75-a3ee-051be89de0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `toDF()` has another signature that takes arguments to define column names as shown below\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03847be0-57f4-48cf-b290-5791c311ab61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.2 Using PySpark `createDataFrame()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160deb47-cdbf-4f7c-8b85-e3453a8a4a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
