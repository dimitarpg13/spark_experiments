{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3159d494-6694-4246-aa08-e2405f5053bb",
   "metadata": {},
   "source": [
    "# PySpark RDD Tutorial \n",
    "\n",
    "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects.\n",
    "Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed\n",
    "on different nodes of the cluster.\n",
    "\n",
    "In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered \n",
    "across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process.\n",
    "\n",
    "Additionally, RDDs provide data abstraction of partitioning and distribution of the data designed to run computations in parallel on several nodes,\n",
    "while doing transformations on RDD we don't have to worry about the parallelism as PySpark by default provides.\n",
    "\n",
    "This PySpark RDD tutorial describes the basic operations available on RDDs, such as `map()` , `filter()` , and `persist()` and many more. In addition, \n",
    "this tutorial also explains Pair RDD functions that operate on RDDs of key-value pairs such as `groupByKey()` and `join()` etc.\n",
    "\n",
    "Note: RDDs can have a name and unique idenitifier (id)\n",
    "\n",
    "## PySpark RDD Benefits\n",
    "\n",
    "PySpark is widely adapated for ML due to its advantages compared to traditional Python programming.\n",
    "\n",
    "## In-Memory Processing\n",
    "\n",
    "PySpark loaded the data from disk in memory and keeps it in-memory. This is the main difference between PySpark and MapREduce which is I/O intensive.\n",
    "In between transformations, we can also cache/persist the RDD in memory to reuse the prvious computations.\n",
    "\n",
    "## Immutability\n",
    "\n",
    "PySpark RDD's are immutable in nature - once RDD is created you cannot modify it. When we apply transformations on RDD, PySpark creates a new RDD and \n",
    "maintains the RDD Lineage.\n",
    "\n",
    "## Fault Tolerance\n",
    "\n",
    "PySpark operates on fault-tolerant data stores on HDFS, S3 etc. If any RDD operation fails, it automatically reloads the data from other partitions. \n",
    "Also when PySpark applications are running on a cluster, PySpark task failures are automatically recovered for a certain number of times (as per the configuration) and finish the application seamlessly.\n",
    "\n",
    "## Lazy Evolution\n",
    "\n",
    "PySpark does not evaluate the RDD transformations as they appear or are encountered by driver. Instead it keeps all transformations as it encounters \n",
    "them (in a DAG) and evaluates all transformation when it sees the first RDD action.\n",
    "\n",
    "## Partitioning\n",
    "\n",
    "When you create RDD from data, by default PySpark by default partitions the elements in RDD. By default the Spark Engine partitions in a number \n",
    "equal to the number of cores available.\n",
    "\n",
    "## PySpark RDD Limitations \n",
    "\n",
    "PySpark RDDs are not much suitable for applications that make updates to the state store such as storage systems for a web application. For these \n",
    "applications, it is more efficient to use systems that perform traditional update logging and data checkpointing, such as databases. The goal of RDD is\n",
    "to provide an efficient programming model for batch analytics and leave these asynchronous applications aside.\n",
    "\n",
    "## Creating RDD\n",
    "\n",
    "RDDs are created primarily in two different ways\n",
    "\n",
    "* by parallelizing an existing collection\n",
    "* by referencing a dataset in an external storage system (HDFS, S3, etc)\n",
    "\n",
    "We start with initialization of SparkSession using the builder pattern method defined in the `SparkSession` class. WHile initializing, we need to provide\n",
    "the master and application name as shown below. In realtime application you will pass master from spark-submit instead of hardcoing on Spark application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044ca4ba-7c2f-4de8-a588-f86621bc0cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark:SparkSession = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExample.com\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80295a-2385-417d-99a1-35ec6e1ec741",
   "metadata": {},
   "source": [
    "`master()` - if you are running it on the cluster you need to use your master name as an argument to master(). Usually, it would be either yarn or mesos\n",
    "depending on your cluster setup. \n",
    "Use `local[x]` when running in Standalone mode. `x` should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame and Dataset. Ideally, `x` value should be the number of CPU cores you have.\n",
    "`appName()` - used to set your application name\n",
    "`getOrCreate()` - this returns a SparkSession object if already exists, and creates a new one if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e453a7a9-1f01-435d-9583-cee7a508d088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4865153d-0ca6-4c48-a179-d6c19c033cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial partition count:1\n"
     ]
    }
   ],
   "source": [
    "print(\"initial partition count:\"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f490ee6-7788-4e97-98ea-2c3a36da609b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
